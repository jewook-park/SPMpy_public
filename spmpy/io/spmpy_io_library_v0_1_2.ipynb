{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f8ff59",
   "metadata": {},
   "source": [
    "## SPMpy I/O Library v0.1.2 (Notebook-paired)\n",
    "\n",
    "This notebook contains the **I/O function set** intended to live under `spmpy/io/`.\n",
    "It is designed to be **paired with a `.py` file via jupytext**.\n",
    "\n",
    "## Goals\n",
    "- Preserve the legacy workflow interface where it makes sense.\n",
    "- Avoid hidden global side effects (especially `os.chdir()`).\n",
    "- Keep I/O responsibilities limited to: **read + standardize to `xarray.Dataset`**.\n",
    "\n",
    "## Included functions\n",
    "- `select_folder()` — GUI folder picker (PyQt5)\n",
    "- `files_in_folder()` — inventory a folder into a `pandas.DataFrame` (**no `chdir`**)\n",
    "- `img2xr()` — load `.sxm` into `xarray.Dataset` (NetCDF-safe attrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da082186",
   "metadata": {},
   "source": [
    "## Why we avoid `os.chdir()`\n",
    "\n",
    "`os.chdir()` changes the **process-wide current working directory**. In a notebook workflow, this can silently\n",
    "affect unrelated cells and libraries that use relative paths.\n",
    "\n",
    "### The design used here\n",
    "- We keep your **working folder** as an explicit variable, e.g. `folder_path`.\n",
    "- We store **full paths** for each file in the inventory DataFrame (`file_path`).\n",
    "\n",
    "### Implication for saving results\n",
    "Yes, this means that **saving should also use explicit paths**. For example:\n",
    "\n",
    "- If you want outputs to go next to the raw data: use `output_dir = folder_path`.\n",
    "- If you want a clean separation: use `output_dir = Path(folder_path) / 'processed'`.\n",
    "\n",
    "In other words, you choose the target folder once (explicitly), then every save uses that folder.\n",
    "This is more reproducible than relying on whatever the current working directory happens to be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c403628",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "These are standard dependencies for the I/O layer.\n",
    "(`nanonispy` is required only when `img2xr()` is called.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440160db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Optional GUI dependency (only needed when select_folder() is used)\n",
    "try:\n",
    "    from PyQt5.QtWidgets import QApplication, QFileDialog\n",
    "except Exception:\n",
    "    QApplication = None\n",
    "    QFileDialog = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee283c",
   "metadata": {},
   "source": [
    "## `select_folder()`\n",
    "\n",
    "Folder picker used in Quickstart Stage-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e09a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_folder() -> str:\n",
    "    \"\"\"Open a folder selection dialog and return the selected folder path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Selected folder path. Empty string if no folder was selected.\n",
    "    \"\"\"\n",
    "    if QApplication is None or QFileDialog is None:\n",
    "        raise ModuleNotFoundError(\n",
    "            \"PyQt5 is required for select_folder(). Install PyQt5 or use a non-GUI path workflow.\"\n",
    "        )\n",
    "\n",
    "    app = QApplication.instance()\n",
    "    if app is None:\n",
    "        import sys\n",
    "        app = QApplication(sys.argv)\n",
    "\n",
    "    file_dialog = QFileDialog()\n",
    "    folder_path = file_dialog.getExistingDirectory(None, \"Select Folder\")\n",
    "    return str(folder_path) if folder_path else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf486950-6c1b-4932-893d-93f1b0661a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3f9c16f",
   "metadata": {},
   "source": [
    "## `files_in_folder()` (no `chdir`)\n",
    "\n",
    "This function inventories a folder and returns a DataFrame with the **same columns** as your legacy workflow:\n",
    "\n",
    "- `group`, `num`, `file_name`, `type`\n",
    "\n",
    "Additionally, it includes two columns that make multi-folder workflows safer:\n",
    "\n",
    "- `folder_path` — the folder that was scanned\n",
    "- `file_path` — full path to each file\n",
    "\n",
    "Because `file_path` is explicit, later stages can load and save deterministically without relying on `os.chdir()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_in_folder(path_input: str, print_all: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Generate a DataFrame listing files in the specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_input : str\n",
    "        Folder path.\n",
    "    print_all : bool, optional\n",
    "        If True, prints the full DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with columns: ['group', 'num', 'file_name', 'type', 'folder_path', 'file_path'].\n",
    "    \"\"\"\n",
    "    folder = Path(path_input)\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder does not exist: {folder}\")\n",
    "\n",
    "    # Keep this informational print (legacy-friendly) WITHOUT changing CWD.\n",
    "    print(\"Current Path =\", os.getcwd())\n",
    "    print(\"Target Folder =\", str(folder))\n",
    "\n",
    "    # Inventory by extension (no chdir)\n",
    "    sxm_files  = sorted([p.name for p in folder.glob('*.sxm')])\n",
    "    grid_files = sorted([p.name for p in folder.glob('*.3ds')])\n",
    "    csv_files  = sorted([p.name for p in folder.glob('*.csv')])\n",
    "    gwy_files  = sorted([p.name for p in folder.glob('*.gwy')])\n",
    "    xlsx_files = sorted([p.name for p in folder.glob('*.xlsx')])\n",
    "    nc_files   = sorted([p.name for p in folder.glob('*.nc')])\n",
    "\n",
    "    def _df_for(files, ext_len, num_slice=True):\n",
    "        rows = []\n",
    "        for fn in files:\n",
    "            if num_slice:\n",
    "                group = fn[:-7]\n",
    "                num = fn[-7:-4]\n",
    "            else:\n",
    "                group = fn[:-ext_len]\n",
    "                num = np.nan\n",
    "            rows.append([group, num, fn])\n",
    "        return pd.DataFrame(rows, columns=['group', 'num', 'file_name'])\n",
    "\n",
    "    file_list_sxm_df  = _df_for(sxm_files,  4, num_slice=True)\n",
    "    file_list_3ds_df  = _df_for(grid_files, 4, num_slice=True)\n",
    "    file_list_csv_df  = _df_for(csv_files,  4, num_slice=True)\n",
    "    file_list_gwy_df  = _df_for(gwy_files,  4, num_slice=False)\n",
    "    file_list_xlsx_df = _df_for(xlsx_files, 5, num_slice=False)\n",
    "    file_list_nc_df   = _df_for(nc_files,   3, num_slice=False)\n",
    "\n",
    "    file_list_df = pd.concat(\n",
    "        [file_list_sxm_df, file_list_3ds_df, file_list_csv_df,\n",
    "         file_list_gwy_df, file_list_xlsx_df, file_list_nc_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    file_list_df['type'] = [fn[-3:] for fn in file_list_df.file_name]\n",
    "    file_list_df.loc[file_list_df.type == 'lsx', 'type'] = 'xlsx'\n",
    "    file_list_df.loc[file_list_df.type == '.nc', 'type'] = 'nc'\n",
    "\n",
    "    # Add explicit paths\n",
    "    file_list_df['folder_path'] = str(folder)\n",
    "    file_list_df['file_path'] = [str(folder / fn) for fn in file_list_df.file_name]\n",
    "\n",
    "    if print_all:\n",
    "        print(file_list_df)\n",
    "\n",
    "    # Legacy-style summary prints\n",
    "    sxm_file_groups = list(set(file_list_sxm_df['group']))\n",
    "    for group in sxm_file_groups:\n",
    "        print('sxm file groups:', group, ': # of files =',\n",
    "              len(file_list_sxm_df[file_list_sxm_df['group'] == group]))\n",
    "\n",
    "    if len(file_list_df[file_list_df['type'] == '3ds']) == 0:\n",
    "        print('No GridSpectroscopy data')\n",
    "    else:\n",
    "        print('# of GridSpectroscopy',\n",
    "              list(set(file_list_df[file_list_df['type'] == '3ds'].group))[0],\n",
    "              '=', file_list_df[file_list_df['type'] == '3ds'].group.count())\n",
    "\n",
    "    return file_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749ce07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7731d06",
   "metadata": {},
   "source": [
    "## `grid2xr()` (3DS → `xarray.Dataset`)\n",
    "\n",
    "This function loads Nanonis GridSpectroscopy `.3ds` files and standardizes them into an `xarray.Dataset`.\n",
    "\n",
    "Design rules:\n",
    "- I/O only: reading + metadata/coords standardization.\n",
    "- No plane fit / flattening / filtering here.\n",
    "\n",
    "If `nanonispy` (or other required dependencies) are missing, the function should raise a clear error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid2xr(griddata_file: str, center_offset: bool = True) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Convert a Nanonis `.3ds` grid spectroscopy file into an `xarray.Dataset`.\n",
    "\n",
    "    This function reads the grid file, constructs spatial coordinates (X, Y) and the\n",
    "    bias axis (`bias_mV`), and exports spectroscopy signals into a labeled dataset.\n",
    "\n",
    "    Key features (kept and clarified from the legacy implementation):\n",
    "    - X/Y coordinates are created from the scan center and ROI size (in meters in the header).\n",
    "    - Bias axis is stored in millivolts as `bias_mV`.\n",
    "    - Forward/backward traces are kept if present (depending on the recorded channels).\n",
    "    - Metadata is stored in `grid_xr.attrs`, including:\n",
    "        * title (requested format)\n",
    "        * image_size, X_spacing, Y_spacing\n",
    "        * optional tip/sample/temperature placeholders (legacy convenience)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    griddata_file\n",
    "        Path to a Nanonis `.3ds` file.\n",
    "    center_offset\n",
    "        If True (default), X and Y coordinates remain centered around the scan center.\n",
    "        If False, X and Y coordinates are shifted so that (X, Y) starts at the ROI lower-left corner.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset with dimensions:\n",
    "        - Y, X: real-space coordinates (meters)\n",
    "        - bias_mV: bias axis in millivolts\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Title convention (as requested):\n",
    "      - includes file name\n",
    "      - includes ROI size in nm x nm\n",
    "      - includes set bias (mV) and setpoint current (pA)\n",
    "      - includes rotation if available in the header: \"R=30deg\"\n",
    "    \"\"\"\n",
    "    import nanonispy as nap\n",
    "\n",
    "    file = griddata_file\n",
    "    NF = nap.read.NanonisFile(file)\n",
    "    Gr = nap.read.Grid(NF.fname)\n",
    "\n",
    "    # ----------------------------- parsing helpers -----------------------------\n",
    "    _num_re = re.compile(r\"([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)\")\n",
    "\n",
    "    def parse_signed_float(v) -> float:\n",
    "        \"\"\"Parse a float from header values that may include units.\"\"\"\n",
    "        if v is None:\n",
    "            return float(\"nan\")\n",
    "        if isinstance(v, (int, float, np.number)):\n",
    "            return float(v)\n",
    "        if isinstance(v, (list, tuple)) and len(v) > 0:\n",
    "            v = v[0]\n",
    "        s = str(v)\n",
    "        m = _num_re.search(s)\n",
    "        if not m:\n",
    "            return float(\"nan\")\n",
    "        return float(m.group(1))\n",
    "\n",
    "    def get_case_insensitive_key(d: dict, candidates: list[str]) -> str | None:\n",
    "        \"\"\"Return the first matching key in d for any candidate key (case-insensitive).\"\"\"\n",
    "        if not isinstance(d, dict):\n",
    "            return None\n",
    "        low = {str(k).lower(): k for k in d.keys()}\n",
    "        for c in candidates:\n",
    "            if c.lower() in low:\n",
    "                return low[c.lower()]\n",
    "        return None\n",
    "\n",
    "    # ----------------------------- geometry / coordinates -----------------------------\n",
    "    dim_px, dim_py = Gr.header[\"dim_px\"]\n",
    "    cntr_x, cntr_y = Gr.header[\"pos_xy\"]          # meters\n",
    "    size_x, size_y = Gr.header[\"size_xy\"]         # meters\n",
    "    step_dx, step_dy = size_x / dim_px, size_y / dim_py\n",
    "\n",
    "    x = np.linspace(cntr_x - size_x / 2, cntr_x + size_x / 2, dim_px)\n",
    "    y = np.linspace(cntr_y - size_y / 2, cntr_y + size_y / 2, dim_py)\n",
    "\n",
    "    # ----------------------------- spectroscopy axes -----------------------------\n",
    "    bias = Gr.signals[\"sweep_signal\"]  # Volts (typical)\n",
    "    bias_mV = 1000.0 * np.array(bias, dtype=float)\n",
    "\n",
    "    # ----------------------------- signals -----------------------------\n",
    "    topography = Gr.signals[\"topo\"]\n",
    "    params_v = Gr.signals[\"params\"]  # (dim_px, dim_py, 15) in legacy format\n",
    "\n",
    "    # Build dataset with standard dims order (Y, X, bias)\n",
    "    grid_xr = xr.Dataset(\n",
    "        coords=dict(\n",
    "            X=(\"X\", x),\n",
    "            Y=(\"Y\", y),\n",
    "            bias_mV=(\"bias_mV\", bias_mV),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Nanonis stores arrays often as (X, Y, ...) so we transpose carefully to (Y, X, ...)\n",
    "    # Topography is usually (dim_px, dim_py)\n",
    "    grid_xr[\"topography\"] = xr.DataArray(np.array(topography).T, dims=(\"Y\", \"X\"), coords={\"Y\": y, \"X\": x})\n",
    "\n",
    "    # Store params as a variable for completeness (legacy behavior)\n",
    "    grid_xr[\"params\"] = xr.DataArray(np.array(params_v).transpose(1, 0, 2), dims=(\"Y\", \"X\", \"params_dim\"),\n",
    "                                     coords={\"Y\": y, \"X\": x})\n",
    "\n",
    "    # Add all spectroscopy channels (excluding topo/params/sweep_signal)\n",
    "    for ch_name in Gr.signals.keys():\n",
    "        if ch_name in (\"topo\", \"params\", \"sweep_signal\"):\n",
    "            continue\n",
    "        arr = np.array(Gr.signals[ch_name])\n",
    "        # Expected shape: (dim_px, dim_py, n_bias) -> transpose to (dim_py, dim_px, n_bias)\n",
    "        if arr.ndim == 3:\n",
    "            arr2 = arr.transpose(1, 0, 2)\n",
    "            grid_xr[ch_name] = xr.DataArray(arr2, dims=(\"Y\", \"X\", \"bias_mV\"),\n",
    "                                            coords={\"Y\": y, \"X\": x, \"bias_mV\": bias_mV})\n",
    "        elif arr.ndim == 2:\n",
    "            arr2 = arr.T\n",
    "            grid_xr[ch_name] = xr.DataArray(arr2, dims=(\"Y\", \"X\"), coords={\"Y\": y, \"X\": x})\n",
    "        else:\n",
    "            # Keep any unexpected shape as-is with minimal labeling\n",
    "            grid_xr[ch_name] = xr.DataArray(arr)\n",
    "\n",
    "    # ----------------------------- coordinate offset convention -----------------------------\n",
    "    if not center_offset:\n",
    "        grid_xr = grid_xr.assign_coords(\n",
    "            X=(grid_xr[\"X\"] + (cntr_x - size_x / 2.0)),\n",
    "            Y=(grid_xr[\"Y\"] + (cntr_y - size_y / 2.0)),\n",
    "        )\n",
    "\n",
    "    # ----------------------------- title (requested format) -----------------------------\n",
    "    basename = os.path.basename(file)\n",
    "    roi_x_nm = float(size_x) * 1e9\n",
    "    roi_y_nm = float(size_y) * 1e9\n",
    "\n",
    "    k_bias = get_case_insensitive_key(Gr.header, [\"bias>bias (v)\", \"bias>bias (V)\", \"bias\"])\n",
    "    k_setpt = get_case_insensitive_key(Gr.header, [\"z-controller>setpoint\", \"z-controller>setpoint (A)\", \"setpoint\"])\n",
    "    V_set = parse_signed_float(Gr.header.get(k_bias)) if k_bias else float(\"nan\")\n",
    "    I_set = parse_signed_float(Gr.header.get(k_setpt)) if k_setpt else float(\"nan\")\n",
    "    if np.isnan(V_set):\n",
    "        V_set = 0.0\n",
    "    if np.isnan(I_set):\n",
    "        I_set = 0.0\n",
    "\n",
    "    bias_set_mV = 1000.0 * float(V_set)\n",
    "    setpoint_pA = 1e12 * float(I_set)\n",
    "\n",
    "    # Optional rotation (header-dependent): search for a plausible rotation/angle key\n",
    "    rot_deg = None\n",
    "    for key in Gr.header.keys():\n",
    "        lk = str(key).lower()\n",
    "        if \"scan_angle\" in lk or (\"scan\" in lk and \"angle\" in lk) or \"rotation\" in lk:\n",
    "            try:\n",
    "                rot_deg = float(parse_signed_float(Gr.header.get(key)))\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    title = f\"{basename}\\n\" \\\n",
    "            f\"{round(roi_x_nm)} nm x {round(roi_y_nm)} nm  \" \\\n",
    "            f\"V = {bias_set_mV:.2f} mV, I = {round(setpoint_pA)} pA\"\n",
    "    if rot_deg is not None and not math.isclose(rot_deg, 0.0):\n",
    "        title += f\"  R = {int(round(rot_deg))}deg\"\n",
    "\n",
    "    # Keep sweep range as a second line (useful for grids)\n",
    "    try:\n",
    "        sweep_start = float(Gr.header.get(\"Bias Spectroscopy>Sweep Start (V)\"))\n",
    "        sweep_end = float(Gr.header.get(\"Bias Spectroscopy>Sweep End (V)\"))\n",
    "        title += f\"\\nSweep: {sweep_start:.3f} V → {sweep_end:.3f} V\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ----------------------------- attrs -----------------------------\n",
    "    grid_xr.attrs[\"title\"] = title\n",
    "    grid_xr.attrs[\"image_size\"] = [float(size_x), float(size_y)]\n",
    "    grid_xr.attrs[\"X_spacing\"] = float(step_dx)\n",
    "    grid_xr.attrs[\"Y_spacing\"] = float(step_dy)\n",
    "    grid_xr.attrs[\"data_vars_list\"] = list(grid_xr.data_vars.keys())\n",
    "\n",
    "    # Legacy convenience placeholders (safe defaults)\n",
    "    if \"tip\" not in grid_xr.attrs:\n",
    "        grid_xr.attrs[\"tip\"] = \"To Be Announced\"\n",
    "    if \"sample\" not in grid_xr.attrs:\n",
    "        grid_xr.attrs[\"sample\"] = \"To Be Announced\"\n",
    "    if \"temperature\" not in grid_xr.attrs:\n",
    "        grid_xr.attrs[\"temperature\"] = \"To Be Announced\"\n",
    "\n",
    "    return grid_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e584f",
   "metadata": {},
   "source": [
    "## `img2xr()` (SXM → `xarray.Dataset`)\n",
    "\n",
    "This is the same `img2xr_updated` logic you provided (robust multipass detection + NetCDF-safe attrs),\n",
    "kept as an I/O-only function.\n",
    "\n",
    "Stage-0 should ensure dependencies are available. If `nanonispy` is missing, this function raises a clear error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0335eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2xr(loading_sxm_file: str, center_offset: bool = True) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Convert a Nanonis `.sxm` image file into an `xarray.Dataset`.\n",
    "\n",
    "    This function is intended to be an I/O utility: it reads a Nanonis image,\n",
    "    builds physically meaningful coordinates (X, Y in meters), and stores\n",
    "    scan metadata in `ds.attrs` in a NetCDF-safe manner.\n",
    "\n",
    "    Key features (restored from the 2025-12-04 implementation):\n",
    "    - Robust multipass detection (header-based and signal-name-based).\n",
    "    - Multipass channel reconstruction into variables like:\n",
    "        * Z_P1_fwd, Z_P1_bwd, LIX_P1_fwd, LIX_P1_bwd, ...\n",
    "      along with `channels_index` and `bias_table` in attributes.\n",
    "    - Strict LIX detection:\n",
    "        * Only signals containing both \"LI\" and \"X\" (case-insensitive) are treated as LIX.\n",
    "        * CURRENT is *not* used as a fallback for LIX.\n",
    "        * CURRENT (if present) is stored separately as CURR_fwd / CURR_bwd.\n",
    "    - NetCDF-safe attribute serialization: dict/list attributes are JSON-serialized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loading_sxm_file\n",
    "        Path to a Nanonis `.sxm` file.\n",
    "    center_offset\n",
    "        If False (default), X and Y coordinates are shifted so that the origin is at the lower-left\n",
    "        corner of the ROI (i.e., Scan offset is applied: `cntr - size/2`).\n",
    "        If True, X and Y coordinates remain centered around the scan center (i.e., no additional shift).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset with:\n",
    "        - coords: X (m), Y (m)\n",
    "        - data_vars: image channels (Z, LIX, LIY, CURR, etc.) in forward/backward directions\n",
    "        - attrs: metadata including title, ROI size, spacing, rotation, multipass tables, etc.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Title convention (as requested):\n",
    "      - includes file name\n",
    "      - includes ROI size in nm x nm\n",
    "      - includes set bias (mV) and setpoint current (pA)\n",
    "      - includes rotation if present: \"R=30deg\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import nanonispy as nap\n",
    "    except ModuleNotFoundError as e:\n",
    "        raise ModuleNotFoundError(\n",
    "            \"nanonispy is required for reading .sxm files. \"\n",
    "            \"Please install dependencies (Stage-0) and restart the kernel.\"\n",
    "        ) from e\n",
    "\n",
    "    # ----------------------------- NetCDF-safe attribute sanitizer -----------------------------\n",
    "    def _sanitize_attr_value(v):\n",
    "        \"\"\"Convert values into NetCDF-friendly primitives (or JSON strings).\"\"\"\n",
    "        if isinstance(v, (bool, np.bool_)):\n",
    "            return int(v)\n",
    "        if isinstance(v, (np.integer,)):\n",
    "            return int(v)\n",
    "        if isinstance(v, (np.floating,)):\n",
    "            return float(v)\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            try:\n",
    "                return json.dumps(v)\n",
    "            except TypeError:\n",
    "                return json.dumps([str(x) for x in v])\n",
    "        if isinstance(v, dict):\n",
    "            try:\n",
    "                return json.dumps(v)\n",
    "            except TypeError:\n",
    "                return json.dumps({str(k): str(val) for k, val in v.items()})\n",
    "        return v\n",
    "\n",
    "    def _sanitize_dataset_attrs(ds: xr.Dataset) -> xr.Dataset:\n",
    "        \"\"\"Return a copy of the dataset with NetCDF-safe attrs.\"\"\"\n",
    "        ds = ds.copy()\n",
    "        clean = {}\n",
    "        for k, v in ds.attrs.items():\n",
    "            clean[str(k)] = _sanitize_attr_value(v)\n",
    "        ds.attrs = clean\n",
    "        return ds\n",
    "\n",
    "    # ----------------------------- parsing helpers -----------------------------\n",
    "    _num_re = re.compile(r\"([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)\")\n",
    "\n",
    "    def parse_signed_float(v, assume_unit: str = \"\") -> float:\n",
    "        \"\"\"\n",
    "        Parse a numeric value from a header entry.\n",
    "\n",
    "        Header values may appear as:\n",
    "          - float/int\n",
    "          - strings like \"100 mV\", \"1.0e-10 A\", \"200pA\"\n",
    "          - or lists/tuples where the first entry is the value string.\n",
    "\n",
    "        Units are *not* fully normalized here; the caller chooses a unit interpretation.\n",
    "        \"\"\"\n",
    "        if v is None:\n",
    "            return float(\"nan\")\n",
    "        if isinstance(v, (int, float, np.number)):\n",
    "            return float(v)\n",
    "        if isinstance(v, (list, tuple)) and len(v) > 0:\n",
    "            v = v[0]\n",
    "        s = str(v)\n",
    "        m = _num_re.search(s)\n",
    "        if not m:\n",
    "            return float(\"nan\")\n",
    "        return float(m.group(1))\n",
    "\n",
    "    def get_case_insensitive_key(d: dict, candidates: list[str]) -> str | None:\n",
    "        \"\"\"Return the first matching key in d for any candidate key (case-insensitive).\"\"\"\n",
    "        if not isinstance(d, dict):\n",
    "            return None\n",
    "        low = {str(k).lower(): k for k in d.keys()}\n",
    "        for c in candidates:\n",
    "            if c.lower() in low:\n",
    "                return low[c.lower()]\n",
    "        return None\n",
    "\n",
    "    def as_float_scalar(x) -> float:\n",
    "        \"\"\"Return a python float from an array scalar / numpy scalar / python numeric.\"\"\"\n",
    "        try:\n",
    "            return float(np.asarray(x).reshape(-1)[0])\n",
    "        except Exception:\n",
    "            return float(x)\n",
    "\n",
    "    def classify_signal_name(sig_name: str) -> tuple[str, int | None]:\n",
    "        \"\"\"\n",
    "        Classify a Nanonis signal name into a (kind, pass_index).\n",
    "\n",
    "        kind is one of: \"Z\", \"LI_X\", \"LI_Y\", \"Current\", \"Other\".\n",
    "        pass_index is parsed from \"...P1...\" patterns (None if not present).\n",
    "        \"\"\"\n",
    "        sk = str(sig_name).upper()\n",
    "        m = re.search(r\"P\\s*(\\d+)\", sk)\n",
    "        pidx = int(m.group(1)) if m else None\n",
    "        if \"Z\" in sk and \"LI\" not in sk:\n",
    "            kind = \"Z\"\n",
    "        elif \"LI\" in sk and \"X\" in sk:\n",
    "            kind = \"LI_X\"\n",
    "        elif \"LI\" in sk and \"Y\" in sk:\n",
    "            kind = \"LI_Y\"\n",
    "        elif \"CURRENT\" in sk:\n",
    "            kind = \"Current\"\n",
    "        else:\n",
    "            kind = \"Other\"\n",
    "        return kind, pidx\n",
    "\n",
    "    # ----------------------------- open file + tolerant header access -----------------------------\n",
    "    NF = nap.read.NanonisFile(loading_sxm_file)\n",
    "    Scan = nap.read.Scan(NF.fname)\n",
    "\n",
    "    # Bias / setpoint parsing (robust to small header variations)\n",
    "    k_bias = get_case_insensitive_key(Scan.header, [\"bias>bias (v)\", \"bias>bias (V)\", \"bias\"])\n",
    "    k_setpt = get_case_insensitive_key(Scan.header, [\"z-controller>setpoint\", \"z-controller>setpoint (A)\", \"setpoint\"])\n",
    "\n",
    "    V_b = parse_signed_float(Scan.header[k_bias], assume_unit=\"V\") if k_bias else np.nan\n",
    "    I_t = parse_signed_float(Scan.header[k_setpt], assume_unit=\"A\") if k_setpt else np.nan\n",
    "    if np.isnan(V_b):\n",
    "        V_b = 0.0\n",
    "    if np.isnan(I_t):\n",
    "        I_t = 0.0\n",
    "\n",
    "    size_x, size_y = Scan.header[\"scan_range\"]        # meters\n",
    "    cntr_x, cntr_y = Scan.header[\"scan_offset\"]       # meters\n",
    "    dim_px, dim_py = Scan.header[\"scan_pixels\"]       # pixels\n",
    "    step_dx, step_dy = size_x / dim_px, size_y / dim_py\n",
    "    Rot_Rad = math.radians(float(Scan.header[\"scan_angle\"]))\n",
    "    scan_dir = Scan.header.get(\"scan_dir\", \"up\")\n",
    "    basename = os.path.basename(getattr(Scan, \"basename\", NF.fname))\n",
    "\n",
    "    # ----------------------------- multipass detection -----------------------------\n",
    "    if \"multipass-config\" in Scan.header.keys():\n",
    "        mp_cfg = Scan.header.get(\"multipass-config\", {})\n",
    "        is_multipass = True\n",
    "    else:\n",
    "        header_keys_lower = {str(k).lower(): k for k in Scan.header.keys()}\n",
    "        mp_header_key = None\n",
    "        for lk, orig in header_keys_lower.items():\n",
    "            if \"multipass\" in lk:\n",
    "                mp_header_key = orig\n",
    "                break\n",
    "        has_mp_cfg = mp_header_key is not None\n",
    "        mp_cfg = Scan.header.get(mp_header_key, {}) if has_mp_cfg else {}\n",
    "        if not isinstance(mp_cfg, dict):\n",
    "            mp_cfg = {}\n",
    "\n",
    "        # Fallback: if any signal name contains P<number>, treat as multipass\n",
    "        has_p = any(re.search(r\"P\\s*\\d+\", str(k), flags=re.I) for k in Scan.signals.keys())\n",
    "        is_multipass = bool(has_mp_cfg or has_p)\n",
    "\n",
    "    # ----------------------------- multipass path -----------------------------\n",
    "    if is_multipass:\n",
    "        # Build bias overrides table if available\n",
    "        bias_map: dict[tuple[int, str], float] = {}\n",
    "        values = mp_cfg.get(\"Bias_override_value\", [])\n",
    "        if isinstance(values, (str, int, float)):\n",
    "            values = [values]\n",
    "        try:\n",
    "            vals = [float(v) for v in values]\n",
    "        except Exception:\n",
    "            # Some files store bias override values as comma-separated strings\n",
    "            vals = []\n",
    "            for v in values:\n",
    "                if v is None:\n",
    "                    continue\n",
    "                for tok in str(v).replace(\";\", \",\").split(\",\"):\n",
    "                    tok = tok.strip()\n",
    "                    if tok:\n",
    "                        try:\n",
    "                            vals.append(float(tok))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "        n_passes = len(vals) // 2 if len(vals) >= 2 else 0\n",
    "        if n_passes > 0:\n",
    "            for k in range(n_passes):\n",
    "                bias_map[(k + 1, \"forward\")] = float(vals[2 * k + 0])\n",
    "                bias_map[(k + 1, \"backward\")] = float(vals[2 * k + 1])\n",
    "\n",
    "        # Coordinates (centered by construction; shift later if requested)\n",
    "        X = np.linspace(-size_x / 2.0, size_x / 2.0, dim_px)\n",
    "        Y = np.linspace(-size_y / 2.0, size_y / 2.0, dim_py)\n",
    "        ds = xr.Dataset(coords={\"X\": X, \"Y\": Y})\n",
    "\n",
    "        # Reconstruct channels:\n",
    "        # - Forward scan: store as-is\n",
    "        # - Backward scan: reverse fast-axis to match forward orientation\n",
    "        for sig_name in Scan.signals.keys():\n",
    "            kind, pidx = classify_signal_name(sig_name)\n",
    "            if pidx is None:\n",
    "                # Multipass signals should include pass index; keep unknowns if present\n",
    "                pidx = 1\n",
    "            sig = Scan.signals[sig_name]\n",
    "            for direction in (\"forward\", \"backward\"):\n",
    "                if direction not in sig:\n",
    "                    continue\n",
    "                arr = np.array(sig[direction])\n",
    "                if direction == \"backward\":\n",
    "                    arr = arr[:, ::-1]\n",
    "                var = None\n",
    "                if kind == \"Z\":\n",
    "                    var = f\"Z_P{pidx}_{'fwd' if direction=='forward' else 'bwd'}\"\n",
    "                elif kind == \"LI_X\":\n",
    "                    var = f\"LIX_P{pidx}_{'fwd' if direction=='forward' else 'bwd'}\"\n",
    "                elif kind == \"LI_Y\":\n",
    "                    var = f\"LIY_P{pidx}_{'fwd' if direction=='forward' else 'bwd'}\"\n",
    "                elif kind == \"Current\":\n",
    "                    var = f\"CURR_P{pidx}_{'fwd' if direction=='forward' else 'bwd'}\"\n",
    "                else:\n",
    "                    # Keep other channels but make names NetCDF-safe-ish\n",
    "                    safe = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", str(sig_name)).strip(\"_\")\n",
    "                    var = f\"{safe}_P{pidx}_{'fwd' if direction=='forward' else 'bwd'}\"\n",
    "                ds[var] = xr.DataArray(arr, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "\n",
    "        # Enforce square pixel spacing by interpolation when needed (matches 2025-12-04 logic)\n",
    "        if not np.isclose(step_dx, step_dy):\n",
    "            ny, nx = ds.sizes[\"Y\"], ds.sizes[\"X\"]\n",
    "            x0, x1 = as_float_scalar(ds[\"X\"].values[0]), as_float_scalar(ds[\"X\"].values[-1])\n",
    "            y0, y1 = as_float_scalar(ds[\"Y\"].values[0]), as_float_scalar(ds[\"Y\"].values[-1])\n",
    "            ratio = step_dy / step_dx\n",
    "            ny_new = max(int(round(ny * ratio)), 1)\n",
    "            X_new = np.linspace(float(x0), float(x1), int(nx))\n",
    "            Y_new = np.linspace(float(y0), float(y1), int(ny_new))\n",
    "            ds = ds.interp(X=X_new, Y=Y_new, method=\"linear\")\n",
    "            eff_dx = (float(X_new[-1]) - float(X_new[0])) / max(len(X_new) - 1, 1)\n",
    "            eff_dy = (float(Y_new[-1]) - float(Y_new[0])) / max(len(Y_new) - 1, 1)\n",
    "        else:\n",
    "            eff_dx, eff_dy = step_dx, step_dy\n",
    "\n",
    "        if not center_offset:\n",
    "            ds = ds.assign_coords(\n",
    "                X=(ds[\"X\"] + (cntr_x - size_x / 2.0)),\n",
    "                Y=(ds[\"Y\"] + (cntr_y - size_y / 2.0)),\n",
    "            )\n",
    "\n",
    "        # ----------------------------- title (requested format) -----------------------------\n",
    "        roi_x_nm = float(size_x) * 1e9\n",
    "        roi_y_nm = float(size_y) * 1e9\n",
    "        bias_mV = 1000.0 * float(V_b)\n",
    "        setpoint_pA = 1e12 * float(I_t)\n",
    "        title = (    f\"{basename}\\n\"  \n",
    "                     f\"{round(roi_x_nm)} nm x {round(roi_y_nm)} nm, \"\n",
    "                     f\"V = {bias_mV:.0f} mV, \"\n",
    "                     f\"I = {round(setpoint_pA)} pA\"\n",
    "                )\n",
    "        if not math.isclose(Rot_Rad, 0.0):\n",
    "            title += f\"  R={int(round(math.degrees(Rot_Rad)))}deg\"\n",
    "\n",
    "        # Multipass per-pass bias summary (if overrides exist)\n",
    "        passes = sorted({int(m.group(1)) for v in ds.data_vars for m in [re.search(r\"P(\\d+)\", v)] if m})\n",
    "        bias_info = []\n",
    "        for p in passes:\n",
    "            bf_mV = 1000.0 * float(bias_map.get((p, \"forward\"), V_b))\n",
    "            bb_mV = 1000.0 * float(bias_map.get((p, \"backward\"), V_b))\n",
    "            bias_info.append(f\"P{p} fwd @{bf_mV:.2f} mV / bwd @{bb_mV:.2f} mV\")\n",
    "        if bias_info:\n",
    "            title += \"\\n\" + \" // \".join(bias_info)\n",
    "\n",
    "        # Minimal tip/sample inference maintained for backward-compat with older notebooks\n",
    "        def infer_tip(title_str: str) -> str:\n",
    "            if \"PTIR\" in title_str.upper(): return \"PtIr\"\n",
    "            if \"WTIP\" in title_str.upper(): return \"W\"\n",
    "            if \"CO_COATED\" in title_str.upper(): return \"Co_coated\"\n",
    "            if \"AFM\" in title_str.upper(): return \"AFM\"\n",
    "            return \"To Be Announced\"\n",
    "\n",
    "        def infer_sample(title_str: str) -> str:\n",
    "            if \"NBSE2\" in title_str.upper(): return \"NbSe2\"\n",
    "            if \"CU(111)\" in title_str.upper(): return \"Cu(111)\"\n",
    "            if \"AU(111)\" in title_str.upper(): return \"Au(111)\"\n",
    "            if \"HOPG\" in title_str.upper(): return \"HOPG\"\n",
    "            return \"To Be Announced\"\n",
    "\n",
    "        # ----------------------------- attrs (restored) -----------------------------\n",
    "        ds.attrs[\"multipass\"] = True\n",
    "        ds.attrs[\"n_passes\"] = int(len(passes)) if passes else 1\n",
    "        ds.attrs[\"title\"] = title\n",
    "        ds.attrs[\"tip\"] = infer_tip(title)\n",
    "        ds.attrs[\"sample\"] = infer_sample(title)\n",
    "        ds.attrs[\"image_size\"] = [float(size_x), float(size_y)]\n",
    "        ds.attrs[\"X_spacing\"] = float(eff_dx)\n",
    "        ds.attrs[\"Y_spacing\"] = float(eff_dy)\n",
    "\n",
    "        ds.attrs[\"channels_index\"] = [\n",
    "            dict(\n",
    "                var=vn,\n",
    "                pass_index=(int(m.group(1)) if (m := re.search(r\"P(\\d+)\", vn)) else None),\n",
    "                dir=(\"forward\" if vn.endswith(\"_fwd\") else \"backward\" if vn.endswith(\"_bwd\") else \"\"),\n",
    "                kind=(\"Z\" if vn.startswith(\"Z\") else\n",
    "                      \"LI_X\" if vn.startswith(\"LIX\") else\n",
    "                      \"LI_Y\" if vn.startswith(\"LIY\") else\n",
    "                      \"Current\" if vn.startswith(\"CURR\") else\n",
    "                      \"Other\"),\n",
    "                bias_mV=(1000.0 * float(\n",
    "                    (lambda p, d: (bias_map.get((p, \"forward\"), V_b) if d == \"forward\"\n",
    "                                   else bias_map.get((p, \"backward\"), V_b)))\n",
    "                    (int(m.group(1)) if (m := re.search(r\"P(\\d+)\", vn)) else 1,\n",
    "                     \"forward\" if vn.endswith(\"_fwd\") else \"backward\")\n",
    "                )),\n",
    "            )\n",
    "            for vn in ds.data_vars\n",
    "        ]\n",
    "\n",
    "        ds.attrs[\"bias_table\"] = [\n",
    "            dict(\n",
    "                pass_index=int(p),\n",
    "                bias_fwd_V=float(bias_map.get((p, \"forward\"), V_b)),\n",
    "                bias_bwd_V=float(bias_map.get((p, \"backward\"), V_b)),\n",
    "            )\n",
    "            for p in passes\n",
    "        ] if passes else [dict(pass_index=1, bias_fwd_V=float(V_b), bias_bwd_V=float(V_b))]\n",
    "\n",
    "        ds.attrs[\"scan_angle_deg\"] = float(math.degrees(Rot_Rad))\n",
    "        ds.attrs[\"scan_dir\"] = str(scan_dir)\n",
    "        ds.attrs[\"data_vars_list\"] = list(ds.data_vars.keys())\n",
    "\n",
    "        ds = _sanitize_dataset_attrs(ds)\n",
    "        return ds\n",
    "\n",
    "    # ----------------------------- single-pass path -----------------------------\n",
    "    z_fwd = np.array(Scan.signals[\"Z\"][\"forward\"]) if \"Z\" in Scan.signals else None\n",
    "    z_bwd = np.array(Scan.signals[\"Z\"][\"backward\"])[:, ::-1] if \"Z\" in Scan.signals else None\n",
    "\n",
    "    # STRICT LIX (no CURRENT fallback)\n",
    "    lix_key = None\n",
    "    for k in Scan.signals.keys():\n",
    "        s = str(k).upper()\n",
    "        if (\"LI\" in s) and (\"X\" in s):\n",
    "            lix_key = k\n",
    "            break\n",
    "    lix_fwd = np.array(Scan.signals[lix_key][\"forward\"]) if lix_key and \"forward\" in Scan.signals[lix_key] else None\n",
    "    lix_bwd = np.array(Scan.signals[lix_key][\"backward\"])[:, ::-1] if lix_key and \"backward\" in Scan.signals[lix_key] else None\n",
    "\n",
    "    liy_key = None\n",
    "    for k in Scan.signals.keys():\n",
    "        s = str(k).upper()\n",
    "        if (\"LI\" in s) and (\"Y\" in s):\n",
    "            liy_key = k\n",
    "            break\n",
    "    liy_fwd = np.array(Scan.signals[liy_key][\"forward\"]) if liy_key and \"forward\" in Scan.signals[liy_key] else None\n",
    "    liy_bwd = np.array(Scan.signals[liy_key][\"backward\"])[:, ::-1] if liy_key and \"backward\" in Scan.signals[liy_key] else None\n",
    "\n",
    "    curr_key = None\n",
    "    for k in Scan.signals.keys():\n",
    "        if \"CURRENT\" in str(k).upper():\n",
    "            curr_key = k\n",
    "            break\n",
    "    curr_fwd = np.array(Scan.signals[curr_key][\"forward\"]) if curr_key and \"forward\" in Scan.signals[curr_key] else None\n",
    "    curr_bwd = np.array(Scan.signals[curr_key][\"backward\"])[:, ::-1] if curr_key and \"backward\" in Scan.signals[curr_key] else None\n",
    "\n",
    "    # Base dataset (centered by construction; shift later if requested)\n",
    "    X = np.linspace(-size_x / 2.0, size_x / 2.0, dim_px)\n",
    "    Y = np.linspace(-size_y / 2.0, size_y / 2.0, dim_py)\n",
    "    ds = xr.Dataset(coords={\"X\": X, \"Y\": Y})\n",
    "\n",
    "    if z_fwd is not None:\n",
    "        ds[\"Z_fwd\"] = xr.DataArray(z_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if z_bwd is not None:\n",
    "        ds[\"Z_bwd\"] = xr.DataArray(z_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if lix_fwd is not None:\n",
    "        ds[\"LIX_fwd\"] = xr.DataArray(lix_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if lix_bwd is not None:\n",
    "        ds[\"LIX_bwd\"] = xr.DataArray(lix_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if liy_fwd is not None:\n",
    "        ds[\"LIY_fwd\"] = xr.DataArray(liy_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if liy_bwd is not None:\n",
    "        ds[\"LIY_bwd\"] = xr.DataArray(liy_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if curr_fwd is not None:\n",
    "        ds[\"CURR_fwd\"] = xr.DataArray(curr_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if curr_bwd is not None:\n",
    "        ds[\"CURR_bwd\"] = xr.DataArray(curr_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "\n",
    "    # Interpolate if pixel aspect ratio differs\n",
    "    if not np.isclose(step_dx, step_dy):\n",
    "        ny, nx = ds.sizes[\"Y\"], ds.sizes[\"X\"]\n",
    "        x0, x1 = as_float_scalar(ds[\"X\"].values[0]), as_float_scalar(ds[\"X\"].values[-1])\n",
    "        y0, y1 = as_float_scalar(ds[\"Y\"].values[0]), as_float_scalar(ds[\"Y\"].values[-1])\n",
    "        ratio = step_dy / step_dx\n",
    "        ny_new = max(int(round(ny * ratio)), 1)\n",
    "        X_new = np.linspace(float(x0), float(x1), int(nx))\n",
    "        Y_new = np.linspace(float(y0), float(y1), int(ny_new))\n",
    "        ds = ds.interp(X=X_new, Y=Y_new, method=\"linear\")\n",
    "        eff_dx = (float(X_new[-1]) - float(X_new[0])) / max(len(X_new) - 1, 1)\n",
    "        eff_dy = (float(Y_new[-1]) - float(Y_new[0])) / max(len(Y_new) - 1, 1)\n",
    "    else:\n",
    "        eff_dx, eff_dy = step_dx, step_dy\n",
    "\n",
    "    # Apply center offset ONLY when requested\n",
    "    # center_offset=True  -> shift to lower-left origin\n",
    "    # center_offset=False -> keep centered coordinates\n",
    "    if center_offset:\n",
    "        ds = ds.assign_coords(\n",
    "            X=(ds[\"X\"] + (cntr_x - size_x / 2.0)),\n",
    "            Y=(ds[\"Y\"] + (cntr_y - size_y / 2.0)),\n",
    "        )\n",
    "\n",
    "    # Title (requested format)\n",
    "    roi_x_nm = float(size_x) * 1e9\n",
    "    roi_y_nm = float(size_y) * 1e9\n",
    "    bias_mV = 1000.0 * float(V_b)\n",
    "    setpoint_pA = 1e12 * float(I_t)\n",
    "    base_title = (\n",
    "        f\"{basename}\\n\"\n",
    "        f\"{round(roi_x_nm)} nm x {round(roi_y_nm)} nm, \"\n",
    "        f\"V = {bias_mV:.0f} mV, \"\n",
    "        f\"I = {round(setpoint_pA)} pA\"\n",
    "    )\n",
    "    if not math.isclose(Rot_Rad, 0.0):\n",
    "        base_title += f\"  R={int(round(math.degrees(Rot_Rad)))}deg\"\n",
    "\n",
    "    # Minimal tip/sample inference maintained for backward-compat with older notebooks\n",
    "    def infer_tip(title_str: str) -> str:\n",
    "        if \"PTIR\" in title_str.upper(): return \"PtIr\"\n",
    "        if \"WTIP\" in title_str.upper(): return \"W\"\n",
    "        if \"CO_COATED\" in title_str.upper(): return \"Co_coated\"\n",
    "        if \"AFM\" in title_str.upper(): return \"AFM\"\n",
    "        return \"To Be Announced\"\n",
    "\n",
    "    def infer_sample(title_str: str) -> str:\n",
    "        if \"NBSE2\" in title_str.upper(): return \"NbSe2\"\n",
    "        if \"CU(111)\" in title_str.upper(): return \"Cu(111)\"\n",
    "        if \"AU(111)\" in title_str.upper(): return \"Au(111)\"\n",
    "        if \"HOPG\" in title_str.upper(): return \"HOPG\"\n",
    "        return \"To Be Announced\"\n",
    "\n",
    "    channels_index = []\n",
    "    for vn in ds.data_vars:\n",
    "        direction = \"forward\" if vn.endswith(\"_fwd\") else \"backward\" if vn.endswith(\"_bwd\") else \"\"\n",
    "        if vn.startswith(\"Z\"): kind = \"Z\"\n",
    "        elif vn.startswith(\"LIX\"): kind = \"LI_X\"\n",
    "        elif vn.startswith(\"LIY\"): kind = \"LI_Y\"\n",
    "        elif vn.startswith(\"CURR\"): kind = \"Current\"\n",
    "        else: kind = \"Other\"\n",
    "        channels_index.append(dict(var=vn, pass_index=1, dir=direction, kind=kind, bias_mV=1000.0 * float(V_b)))\n",
    "\n",
    "    ds.attrs[\"multipass\"] = False\n",
    "    ds.attrs[\"n_passes\"] = 1\n",
    "    ds.attrs[\"title\"] = base_title\n",
    "    ds.attrs[\"tip\"] = infer_tip(base_title)\n",
    "    ds.attrs[\"sample\"] = infer_sample(base_title)\n",
    "    ds.attrs[\"image_size\"] = [float(size_x), float(size_y)]\n",
    "    ds.attrs[\"X_spacing\"] = float(eff_dx)\n",
    "    ds.attrs[\"Y_spacing\"] = float(eff_dy)\n",
    "    ds.attrs[\"channels_index\"] = channels_index\n",
    "    ds.attrs[\"bias_table\"] = [dict(pass_index=1, bias_fwd_V=float(V_b), bias_bwd_V=float(V_b))]\n",
    "    ds.attrs[\"scan_angle_deg\"] = float(math.degrees(Rot_Rad))\n",
    "    ds.attrs[\"scan_dir\"] = str(scan_dir)\n",
    "    ds.attrs[\"data_vars_list\"] = list(ds.data_vars.keys())\n",
    "\n",
    "    ds = _sanitize_dataset_attrs(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c155f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
